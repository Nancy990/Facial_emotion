import streamlit as st
import cv2
import numpy as np
import pandas as pd
import pickle
from PIL import Image
import io

def load_model():
    """Load the trained emotion detection model"""
    # Try to load the new OpenCV-optimized model first
    model_files = ['opencv_emotion_model.pkl', 'Facial_emotion.pkl']
    
    for model_file in model_files:
        try:
            with open(model_file, 'rb') as f:
                model = pickle.load(f)
            st.success(f"âœ… Loaded model: {model_file}")
            return model, model_file
        except FileNotFoundError:
            continue
    
    st.error("âŒ No model file found. Please ensure you have either 'opencv_emotion_model.pkl' or 'Facial_emotion.pkl' in the directory.")
    return None, None

def extract_opencv_features(image):
    """Extract comprehensive OpenCV-based features from an image"""
    if isinstance(image, Image.Image):
        image = np.array(image)
    
    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    
    # Load cascades
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
    smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
    
    # Detect faces
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
    
    if len(faces) == 0:
        return None, "No face detected in the image. Please upload an image with a clear face."
    
    # Use the largest face
    face = max(faces, key=lambda x: x[2] * x[3])
    x, y, w, h = face
    
    # Extract face region
    face_roi = gray[y:y+h, x:x+w]
    
    # Extract comprehensive features
    features = []
    
    # 1. Face geometry features
    img_h, img_w = gray.shape
    features.extend([
        x / img_w,                    # face_x_norm
        y / img_h,                    # face_y_norm
        w / img_w,                    # face_width_norm
        h / img_h,                    # face_height_norm
        w / h,                        # face_aspect_ratio
        (x + w/2) / img_w,           # face_center_x_norm
        (y + h/2) / img_h,           # face_center_y_norm
    ])
    
    # 2. Eyes features
    eyes = eye_cascade.detectMultiScale(face_roi)
    num_eyes = len(eyes)
    features.extend([
        num_eyes,                     # num_eyes_detected
        np.mean([eye[0] for eye in eyes]) / w if num_eyes > 0 else 0,  # avg_eye_x_ratio
        np.mean([eye[1] for eye in eyes]) / h if num_eyes > 0 else 0,  # avg_eye_y_ratio
        np.mean([eye[2] for eye in eyes]) / w if num_eyes > 0 else 0,  # avg_eye_width_ratio
        np.mean([eye[3] for eye in eyes]) / h if num_eyes > 0 else 0,  # avg_eye_height_ratio
    ])
    
    # 3. Smile features
    smiles = smile_cascade.detectMultiScale(face_roi, 1.8, 20)
    num_smiles = len(smiles)
    features.extend([
        num_smiles,                   # num_smiles_detected
        np.mean([smile[0] for smile in smiles]) / w if num_smiles > 0 else 0,  # avg_smile_x_ratio
        np.mean([smile[1] for smile in smiles]) / h if num_smiles > 0 else 0,  # avg_smile_y_ratio
        np.mean([smile[2] for smile in smiles]) / w if num_smiles > 0 else 0,  # avg_smile_width_ratio
        np.mean([smile[3] for smile in smiles]) / h if num_smiles > 0 else 0,  # avg_smile_height_ratio
    ])
    
    # 4. Intensity and contrast features
    features.extend([
        np.mean(face_roi),            # face_mean_intensity
        np.std(face_roi),             # face_std_intensity
        np.min(face_roi),             # face_min_intensity
        np.max(face_roi),             # face_max_intensity
        np.mean(face_roi[:h//3]),     # upper_face_intensity
        np.mean(face_roi[h//3:2*h//3]), # middle_face_intensity
        np.mean(face_roi[2*h//3:]),   # lower_face_intensity
        np.mean(face_roi[:, :w//3]),  # left_face_intensity
        np.mean(face_roi[:, w//3:2*w//3]), # center_face_intensity
        np.mean(face_roi[:, 2*w//3:]),# right_face_intensity
    ])
    
    # 5. Edge and texture features
    edges = cv2.Canny(face_roi, 50, 150)
    features.extend([
        np.sum(edges) / (w * h),      # edge_density
        np.mean(edges),               # avg_edge_intensity
        np.std(edges),                # edge_std
        np.sum(edges[:h//2]) / np.sum(edges) if np.sum(edges) > 0 else 0,  # upper_edge_ratio
        np.sum(edges[h//2:]) / np.sum(edges) if np.sum(edges) > 0 else 0,  # lower_edge_ratio
    ])
    
    # 6. Gradient features
    grad_x = cv2.Sobel(face_roi, cv2.CV_64F, 1, 0, ksize=3)
    grad_y = cv2.Sobel(face_roi, cv2.CV_64F, 0, 1, ksize=3)
    features.extend([
        np.mean(np.abs(grad_x)),      # mean_gradient_x
        np.mean(np.abs(grad_y)),      # mean_gradient_y
        np.std(grad_x),               # std_gradient_x
        np.std(grad_y),               # std_gradient_y
    ])
    
    # 7. Histogram features
    hist = cv2.calcHist([face_roi], [0], None, [16], [0, 256])
    hist_norm = hist.flatten() / np.sum(hist)
    features.extend(hist_norm.tolist())  # 16 histogram bins
    
    # 8. Local features (grid-based)
    grid_h, grid_w = 4, 4
    cell_h, cell_w = max(1, h // grid_h), max(1, w // grid_w)
    
    for i in range(grid_h):
        for j in range(grid_w):
            start_y, end_y = i * cell_h, min((i + 1) * cell_h, h)
            start_x, end_x = j * cell_w, min((j + 1) * cell_w, w)
            cell = face_roi[start_y:end_y, start_x:end_x]
            
            if cell.size > 0:
                features.extend([
                    np.mean(cell),        # cell_mean
                    np.std(cell),         # cell_std
                ])
            else:
                features.extend([0, 0])
    
    return features, None

def predict_emotion_enhanced_fallback(image):
    """Enhanced rule-based emotion prediction"""
    if isinstance(image, Image.Image):
        image = np.array(image)
    
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    
    # Load cascades
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
    
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
    
    if len(faces) == 0:
        return "Normal", [0.25, 0.25, 0.25, 0.25]
    
    # Enhanced heuristic-based emotion detection
    x, y, w, h = faces[0]
    face_roi = gray[y:y+h, x:x+w]
    
    # Detect features
    smiles = smile_cascade.detectMultiScale(face_roi, 1.8, 20)
    eyes = eye_cascade.detectMultiScale(face_roi)
    
    # Calculate additional metrics
    mean_intensity = np.mean(face_roi)
    std_intensity = np.std(face_roi)
    upper_face = np.mean(face_roi[:h//2])
    lower_face = np.mean(face_roi[h//2:])
    
    # Enhanced rules
    smile_score = len(smiles) * 0.3
    eye_score = min(len(eyes), 2) * 0.2
    brightness_score = (mean_intensity - 100) / 100  # Normalized brightness
    contrast_score = std_intensity / 50  # Normalized contrast
    face_balance = abs(upper_face - lower_face) / mean_intensity
    
    # Calculate emotion probabilities
    happy_prob = max(0, smile_score + brightness_score * 0.2 + eye_score)
    stressed_prob = max(0, face_balance * 0.5 + (1 - brightness_score) * 0.3)
    doubt_prob = max(0, contrast_score * 0.3 + (1 - eye_score) * 0.2)
    normal_prob = 1 - max(happy_prob, stressed_prob, doubt_prob)
    
    # Normalize probabilities
    total = happy_prob + stressed_prob + doubt_prob + normal_prob
    if total > 0:
        probabilities = [doubt_prob/total, happy_prob/total, normal_prob/total, stressed_prob/total]
    else:
        probabilities = [0.25, 0.25, 0.25, 0.25]
    
    # Determine prediction
    emotions = ['Doubt', 'Happy', 'Normal', 'Stressed']
    prediction = emotions[np.argmax(probabilities)]
    
    return prediction, probabilities

def draw_face_detection(image):
    """Draw face detection rectangles and features on image"""
    if isinstance(image, Image.Image):
        image = np.array(image)
    
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
    smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
    
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
    
    # Draw rectangles around faces
    for (x, y, w, h) in faces:
        cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)
        cv2.putText(image, 'Face', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)
        
        # Draw eyes in face region
        face_roi = gray[y:y+h, x:x+w]
        eyes = eye_cascade.detectMultiScale(face_roi)
        for (ex, ey, ew, eh) in eyes:
            cv2.rectangle(image, (x+ex, y+ey), (x+ex+ew, y+ey+eh), (0, 255, 0), 2)
        
        # Draw smiles in face region
        smiles = smile_cascade.detectMultiScale(face_roi, 1.8, 20)
        for (sx, sy, sw, sh) in smiles:
            cv2.rectangle(image, (x+sx, y+sy), (x+sx+sw, y+sy+sh), (0, 0, 255), 2)
    
    return image

def main():
    st.title("ğŸ­ Enhanced OpenCV Emotion Detection")
    st.write("Upload an image and let our AI detect emotions using advanced face analysis!")
    
    # Load model
    model, model_file = load_model()
    
    if model is None:
        st.info("ğŸ’¡ **Tip:** For better accuracy, retrain the model specifically for OpenCV features using the training script.")
        st.stop()
    
    # Show model info
    if model_file == 'opencv_emotion_model.pkl':
        st.success("ğŸ¯ Using OpenCV-optimized model for better accuracy!")
    else:
        st.warning("âš ï¸ Using original MediaPipe model. Consider retraining for better accuracy with OpenCV features.")
    
    # File uploader
    uploaded_file = st.file_uploader(
        "Choose an image...", 
        type=['jpg', 'jpeg', 'png', 'bmp'],
        help="For best results, use images with clear, front-facing faces"
    )
    
    if uploaded_file is not None:
        # Display uploaded image
        image = Image.open(uploaded_file)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Original Image")
            st.image(image, caption="Uploaded Image", use_container_width=True)
        
        # Process the image
        with st.spinner("Analyzing facial features..."):
            # Extract features
            features, error = extract_opencv_features(image)
            
            if error:
                st.error(error)
                st.info("ğŸ’¡ **Tips for better results:**")
                st.write("- Use images with clear, front-facing faces")
                st.write("- Ensure good lighting")
                st.write("- Avoid sunglasses or face coverings")
                st.write("- Try images where the face takes up a good portion of the frame")
            else:
                try:
                    # Make prediction with trained model
                    X = pd.DataFrame([features])
                    prediction = model.predict(X)[0]
                    probabilities = model.predict_proba(X)[0]
                    
                    model_success = True
                    method_used = "Machine Learning Model"
                    
                except Exception as e:
                    st.warning(f"âš ï¸ ML model failed: {str(e)}. Using enhanced fallback method.")
                    prediction, probabilities = predict_emotion_enhanced_fallback(image)
                    model_success = False
                    method_used = "Enhanced Rule-Based Detection"
                
                with col2:
                    st.subheader("Analysis Results")
                    
                    # Draw face detection with features
                    annotated_image = draw_face_detection(image.copy())
                    st.image(annotated_image, caption="Detected Features", use_container_width=True)
                
                # Display results
                st.success(f"ğŸ¯ **Detected Emotion: {prediction}**")
                
                # Show method used
                if model_success:
                    st.info(f"ğŸ“Š Method: {method_used} | Confidence: {np.max(probabilities):.2%}")
                else:
                    st.info(f"ğŸ“Š Method: {method_used}")
                
                # Display all class probabilities
                st.subheader("ğŸ“ˆ Emotion Probability Distribution")
                
                # Get class names (adjust based on your model)
                if model_file == 'opencv_emotion_model.pkl':
                    # If using retrained model, classes might be different
                    try:
                        class_names = model.classes_.tolist() if hasattr(model, 'classes_') else ['Doubt', 'Happy', 'Normal', 'Stressed']
                    except:
                        class_names = ['Doubt', 'Happy', 'Normal', 'Stressed']
                else:
                    class_names = ['Doubt', 'Happy', 'Normal', 'Stressed']
                
                # Ensure we have the right number of probabilities
                if len(probabilities) != len(class_names):
                    st.warning("âš ï¸ Probability mismatch. Using generic labels.")
                    class_names = [f'Class_{i}' for i in range(len(probabilities))]
                
                # Create a dataframe for better visualization
                prob_df = pd.DataFrame({
                    'Emotion': class_names,
                    'Probability': probabilities
                }).sort_values('Probability', ascending=False)
                
                # Display as bar chart
                st.bar_chart(prob_df.set_index('Emotion'))
                
                # Display as table with confidence indicators
                prob_df['Confidence'] = prob_df['Probability'].apply(
                    lambda x: "ğŸ”¥ Very High" if x > 0.7 
                    else "âœ… High" if x > 0.5 
                    else "âš ï¸ Medium" if x > 0.3 
                    else "âŒ Low"
                )
                
                st.dataframe(prob_df, use_container_width=True)
                
                # Feature analysis
                if features and model_success:
                    with st.expander("ğŸ” Detailed Feature Analysis"):
                        st.write("**Key facial features detected:**")
                        
                        # Parse some key features (first few are geometric)
                        if len(features) >= 20:
                            col_a, col_b = st.columns(2)
                            
                            with col_a:
                                st.metric("Face Width Ratio", f"{features[2]:.3f}")
                                st.metric("Face Height Ratio", f"{features[3]:.3f}")
                                st.metric("Eyes Detected", int(features[7]))
                                st.metric("Smiles Detected", int(features[12]))
                            
                            with col_b:
                                st.metric("Face Brightness", f"{features[17]:.1f}")
                                st.metric("Face Contrast", f"{features[18]:.1f}")
                                st.metric("Edge Density", f"{features[27]:.4f}")
                                st.metric("Feature Asymmetry", f"{features[30]:.3f}" if len(features) > 30 else "N/A")
                        
                        st.write(f"**Total features analyzed:** {len(features)}")
    
    # Model information and tips
    with st.expander("â„¹ï¸ About This Detection System"):
        st.write("""
        ### ğŸ”¬ **Advanced OpenCV-Based Emotion Detection**
        
        This system analyzes facial features using computer vision to predict emotions:
        
        **ğŸ¯ Detection Features:**
        - **Geometric Analysis**: Face dimensions, position, and proportions
        - **Feature Detection**: Eyes, smiles, and facial landmarks
        - **Intensity Analysis**: Brightness patterns across different face regions
        - **Texture Analysis**: Edge detection and gradient analysis
        - **Statistical Features**: Histograms and local pattern analysis
        
        **ğŸ“ˆ Supported Emotions:**
        - **Normal**: Neutral or calm expression
        - **Happy**: Smiling or joyful expression  
        - **Doubt**: Questioning or uncertain expression
        - **Stressed**: Tense or worried expression
        
        **ğŸ”§ How It Works:**
        1. **Face Detection**: Uses OpenCV Haar cascades to locate faces
        2. **Feature Extraction**: Analyzes 80+ facial characteristics
        3. **Classification**: Machine learning model predicts emotion
        4. **Fallback System**: Rule-based detection if ML model fails
        
        **âš¡ Performance Notes:**
        - **Optimized Model**: Best results with retrained OpenCV-specific model
        - **Fallback Mode**: Enhanced rule-based system as backup
        - **Real-time Analysis**: Fast processing suitable for web deployment
        
        **ğŸ’¡ For Best Results:**
        - Use clear, well-lit photos
        - Face should be front-facing and unobstructed  
        - Avoid heavy shadows or extreme lighting
        - Images where face occupies 20-50% of frame work best
        
        **ğŸš€ Accuracy Improvement:**
        To get better accuracy, consider:
        1. Retraining the model with OpenCV features using the training script
        2. Collecting more diverse training data
        3. Using the webcam data collection method for your specific use case
        """)
    
    # Training information
    with st.expander("ğŸ“ Want Better Accuracy? Retrain the Model"):
        st.write("""
        ### ğŸ“š **Model Retraining Guide**
        
        The current system works but can be significantly improved by retraining with OpenCV-specific features:
        
        **ğŸ”„ Retraining Options:**
        
        **Option 1: Image Folders** (Recommended)
        ```
        training_images/
        â”œâ”€â”€ Happy/
        â”‚   â”œâ”€â”€ image1.jpg
        â”‚   â”œâ”€â”€ image2.jpg
        â”œâ”€â”€ Sad/
        â”‚   â”œâ”€â”€ image1.jpg
        â”œâ”€â”€ Normal/
        â”‚   â”œâ”€â”€ image1.jpg
        â””â”€â”€ Stressed/
            â”œâ”€â”€ image1.jpg
        ```
        
        **Option 2: Webcam Collection**
        - Use the training script to collect live data
        - Express each emotion while recording
        - Automatic feature extraction and model training
        
        **ğŸ“ Steps:**
        1. Download the model retraining script
        2. Organize your training images or use webcam collection
        3. Run the script to generate `opencv_emotion_model.pkl`
        4. Replace the old model file
        5. Deploy the improved version!
        
        **ğŸ¯ Expected Improvements:**
        - **Accuracy**: 60-80% â†’ 85-95%
        - **Consistency**: More reliable predictions
        - **Speed**: Faster inference with optimized features
        """)

if __name__ == "__main__":
    main()
